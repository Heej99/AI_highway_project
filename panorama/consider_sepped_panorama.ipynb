{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b9d3da1-2982-42dd-bd9e-3bc98f7c4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Sep  8 10:13:55 2021\n",
    "\n",
    "@author: KONIDE\n",
    "\n",
    "코드 설명 \n",
    "- 파노라마 생성에 필요한 코드 정의 \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##----------------Stitcher------------------\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils # imutils 는 OpenCV가 제공하는 기능 중에 좀 복잡하고 사용성이 떨어지는 부분을 잘 보완해 주는 패키지\n",
    "               # 기본적으로 모두 OpenCV의 기능을 사용하고 있기 때문에 해당 기능을 사용하는 것은 아주 권장\n",
    "\n",
    "class Line : \n",
    "    \"\"\"\n",
    "    투시변환할 부분을 찾아주는 class\n",
    "    \"\"\"\n",
    "    def __init__(self, data1, data2):\n",
    "   \n",
    "        self.line1 = data1\n",
    "        self.line2 = data2\n",
    "        #print(self.line1)\n",
    "    def slope(self):\n",
    "        (x1, y1), (x2, y2) = self.line1\n",
    "        (x3, y3), (x4, y4) = self.line2\n",
    "        \n",
    "        if (y2-y1) == 0 :\n",
    "            #print('Ys are equal, m1 = 0')\n",
    "            m1 = 0\n",
    "        else:\n",
    "            m1 = (float(y2)-y1)/(float(x2)-x1)\n",
    "        \n",
    "        if (y4-y3) == 0 :\n",
    "            #print('Ys are equal, m2 = 0')\n",
    "            m2 = 0\n",
    "        else:\n",
    "            m2 = (float(y4)-y3)/(float(x4)-x3)\n",
    "            \n",
    "        return m1, m2\n",
    "                    \n",
    "    def yintercept(self, m1, m2):\n",
    "        (x1, y1), (x2, y2) = self.line1\n",
    "        (x3, y3), (x4, y4) = self.line2\n",
    "        \n",
    "        if m1 != 0 :\n",
    "            b1 = y1 - m1*x1\n",
    "        else :\n",
    "            b1 = y1\n",
    "            \n",
    "        if m2 != 0 :\n",
    "            b2 = y4 - m2*x4\n",
    "            \n",
    "        else: b2 = y4\n",
    "        \n",
    "        return b1, b2\n",
    "    \n",
    "    def findIntersect(self, m1,m2, b1, b2):\n",
    "        \n",
    "        if m1 != 0 | m2 != 0 :\n",
    "            px = (b2-b1) / (m1-m2)\n",
    "            py = (b2*m1 - b1*m2)/(m1-m2)\n",
    "        elif m1 == 0 :\n",
    "            px = (b1-b2)/m2\n",
    "            py = b1\n",
    "        elif m2 == 0 : \n",
    "            px = (b2-b1)/m1\n",
    "            py = b2 \n",
    "        else :  print('No points')\n",
    "        \n",
    "        return px, py\n",
    "        \n",
    "\n",
    "class IMP: \n",
    "    \n",
    "    \"\"\"\n",
    "    투시변환 실시하여 전경이미지 -> 정사영으로 변환해주는 class\n",
    "    \"\"\"\n",
    "    def __init__(self, img):\n",
    "        \n",
    "        #import cv2\n",
    "        #img = cv2.imread('c:/OpenCV/image-003.jpeg')     \n",
    "        self.img = img\n",
    "        \n",
    "        #self.topHeight = 565\n",
    "        #self.height, self.width = 1080, 1920\n",
    "        \n",
    "    def impTransformer(self):  \n",
    "        \n",
    "        import numpy as np\n",
    "        import cv2 \n",
    "        \n",
    "        topHeight = 565\n",
    "        height, width = self.img.shape[:2]\n",
    "        #print('height', height,'width :', width)\n",
    "        left = [(960, 380), (0, 650)]\n",
    "        right = [(960, 380), (1920, 650)]\n",
    "        up =  [(0, topHeight), (width+1000, topHeight)]\n",
    "        down =  [(-10000,height), (width+100000, height)]\n",
    "               \n",
    "        leftup = Line(left, up)\n",
    "        leftdown = Line(left, down)\n",
    "        rightup = Line(right, up)\n",
    "        rightdown = Line(right, down)\n",
    "        m1, m2 = leftup.slope()\n",
    "        b1, b2 = leftup.yintercept(m1,m2)\n",
    "        p1x, p1y = leftup.findIntersect(m1,m2,b1,b2)\n",
    "        \n",
    "        #print('point1 : ', p1x, p1y)\n",
    "        \n",
    "       \n",
    "        \n",
    "        m1, m2 = leftdown.slope()\n",
    "        b1, b2 = leftdown.yintercept(m1,m2)\n",
    "        p2x, p2y = leftdown.findIntersect(m1,m2,b1,b2)\n",
    "        #print('point2 : ', p2x, p2y)\n",
    "        \n",
    "       \n",
    "        \n",
    "        m1, m2 = rightup.slope()\n",
    "        b1, b2 = rightup.yintercept(m1,m2)\n",
    "        p3x, p3y = rightup.findIntersect(m1,m2,b1,b2)\n",
    "        #print('point3 : ', p3x, p3y)\n",
    "        \n",
    "        m1, m2 = rightdown.slope()\n",
    "        b1, b2 = rightdown.yintercept(m1,m2)\n",
    "        p4x, p4y = leftup.findIntersect(m1,m2,b1,b2)\n",
    "        #print('point4 : ', p4x, p4y)\n",
    "         \n",
    "        dst = np.array([[0,0], [0, 565], [1080,0], [1080,565]], dtype=np.float32)\n",
    "        src = np.array([ [p1x,p1y], [p2x,p2y], [p3x,p3y], [p4x,p4y]], dtype=np.float32)\n",
    "        mtrx = cv2.getPerspectiveTransform(src, dst) #4개의 꼭짓점으로 정확한 원근 변환 행렬을 반환\n",
    "        \n",
    "        outimg = cv2.warpPerspective(self.img, mtrx, (1080,560))\n",
    "        #cv2.imshow('out_image',outimg)\n",
    "        #cv2.waitKey()\n",
    "        #cv2.destroyAllWindows()\n",
    "        return outimg\n",
    "\n",
    "\n",
    "class Stitcher: \n",
    "    \n",
    "    \"\"\"\n",
    "    정사영 이미지들을 특징매칭하여 파노라마를 생성하는 class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "\t\t# determine if we are using OpenCV v3.X and initialize the\n",
    "\t\t# cached homography matrix\n",
    "        self.isv3 = imutils.is_cv3()\n",
    "        self.cachedH = None\n",
    "        \n",
    "    def stitch(self, images, ratio=0.75, reprojThresh=4.0):\n",
    "\t\t# unpack the images\n",
    "        (imageB, imageA) = images\n",
    "        \n",
    "\t\t# if the cached homography matrix is None, then we need to\n",
    "\t\t# apply keypoint matching to construct it\n",
    "        if self.cachedH is None:\n",
    "\t\t\t# detect keypoints and extract\n",
    "            (kpsA, featuresA) = self.detectAndDescribe(imageA)\n",
    "            (kpsB, featuresB) = self.detectAndDescribe(imageB)\n",
    "\t\t# match features between the two images\n",
    "            M = self.matchKeypoints(kpsA, kpsB,featuresA, featuresB, ratio, reprojThresh)\n",
    "            \n",
    "\t\t\t# if the match is None, then there aren't enough matched\n",
    "\t\t\t# keypoints to create a panorama\n",
    "            if M is None:\n",
    "                return None\n",
    "\t\t\t# cache the homography matrix\n",
    "            self.cachedH = M[1]\n",
    "\t\t# apply a perspective transform to stitch the images together\n",
    "\t\t# using the cached homography matrix\n",
    "        result = cv2.warpPerspective(imageA, self.cachedH,(imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "        \n",
    "        #result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "\t\t# return the stitched image\n",
    "        return result\n",
    "        \n",
    "    def detectAndDescribe(self, image):\n",
    "        \"\"\"\n",
    "        특징점과 특징 디스크립터를 찾아주는 함수\n",
    "            - 특징점 :  특징점은 영어로 키 포인트(Keypoints)라고도 합니다. 보통 특징점이 되는 부분은 물체의 모서리나 코너\n",
    "            - 특징 디스크립터 : 특징점 주변 픽셀을 일정한 크기의 블록으로 나누어 각 블록에 속한 픽셀의 그레디언트 히스토그램을 계산한 것. \n",
    "                                주로 특징점 주변의 밝기, 색상, 방향, 크기 등의 정보가 포함되어 있\n",
    "        \"\"\"\n",
    "\t\t# convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\t\t# check to see if we are using OpenCV 3.X\n",
    "        if self.isv3:\n",
    "\t\t\t# detect and extract features from the image\n",
    "            descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "            (kps, features) = descriptor.detectAndCompute(image, None) # 각 이미지에 대해 키포인트와 디스크립터 추출\n",
    "                                                                       # 디스크립터 : keypoint에 해당하는 정보 <- 실제 유사도를 판별하기 위한 데이터로 활용됨.            \n",
    "\t\t# otherwise, we are using OpenCV 2.4.X\n",
    "        else:\n",
    "\t\t\t# detect keypoints in the image\n",
    "            detector = cv2.SIFT_create()\n",
    "            kps = detector.detect(gray)\n",
    "\t\t\t# extract features from the image\n",
    "            # extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "            (kps, features) = detector.compute(gray, kps)\n",
    "            \n",
    "\t\t# convert the keypoints from KeyPoint objects to NumPy\n",
    "\t\t# arrays\n",
    "        kps = np.float32([kp.pt for kp in kps]) # kp.pt : 특징점의 좌표\n",
    "\t\t# return a tuple of keypoints and features \n",
    "        return (kps, features)\n",
    "    \n",
    "    \n",
    "    def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB,ratio, reprojThresh):\n",
    "        \"\"\"\n",
    "        특징 매칭된 keypoint 찾아주는 알고리즘\n",
    "        - 특징 매칭 : 서로 다른 두 이미지에서 특징점과 특징 디스크립터들을 비교해서 비슷한 객체끼리 짝짓는 것\n",
    "        \"\"\"\n",
    "\t\t# compute the raw matches and initialize the list of actual\n",
    "\t\t# matches\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\") # 원하는 매칭 알고리즘이 BruteForse인 특징 매칭기\n",
    "                                                             # Brute-Forse 매칭기 queryDescriptor와 trainDescriptor를 일일이 전수조사해서 매칭하는 알고리즘\n",
    "            \n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)  # 여러개의 특징 매칭점들(BruteForse에 의해 전수조사됨)에 대한 k개의 근접 이웃 개수 = 각 행에 거리 값이 작은 순서대로 리스트에 추가\n",
    "        # queryDescriptors 한개당 최근접 이웃 개수 만큼 trainDescriptor 한개 찾아 결과 반영,최적 매칭 없을 수도 있음, DMatch 객체 리스트 \n",
    "        # k의 값에 따라 결과 매칭의 각행에 거리 값이 작은 순서대로 리스트에 추가\n",
    "        \n",
    "        '''        \n",
    "        # knnMatch(queryDescriptors, trainDescriptors, k) <- 특징 매칭 방식으로 채택된 것\n",
    "        \n",
    "        # k(매칭할 근접 이웃 개수)개의 가장 근접한 매칭\n",
    "        # queryDescriptors : 매칭의 기준이 될 특징 디스크립터 배열 - queryInx : queryDescriptors의 인덱스\n",
    "        # trainDescriptors : 매칭의 대상이 될 특징 디스크립터 배열 - trainIdx : trainDescriptors의 인덱스\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        matches = []\n",
    "\t\t# loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "\t\t\t# ensure the distance is within a certain ratio of each\n",
    "\t\t\t# other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio: matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "            # m.distance : 유사도 거리\n",
    "    \n",
    "\t# computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "\t\t\t# construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i] for (_, i) in matches]) # _ :  의미없는 변수\n",
    "            ptsB = np.float32([kpsB[i] for (i, _) in matches])\n",
    "\t\t\t# compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,reprojThresh) # 여러 개의 점으로 근사 계산한 원근 변환 행렬 반환\n",
    "            \n",
    "\t\t\t# return the matches along with the homograpy matrix\n",
    "\t\t\t# and status of each matched point\n",
    "        return (matches, H, status)\n",
    "\t\t# otherwise, no homograpy could be computed\n",
    " \n",
    "    def makeImagesList(self):       \n",
    "        \"\"\"\n",
    "        비디오에서 프레임 뽑아서 정사영 형태로 변환한 이미지들을 리스트에 저장하는 역할. \n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture('C:/OpenCV/GH021047.MP4') # 동영상 파일을 읽어옴\n",
    "        \n",
    "        #startFrame = 100 + skipSeconds*60        \n",
    "        fps = round(cap.get(cv2.CAP_PROP_FPS)) # get frame numbers = 프레임속도\n",
    "                    # cap.get(propid) # 동영상 속성 반환\n",
    "        \n",
    "        #delay = int(5000/fps)\n",
    "        \n",
    "        if (fps == 0) :\n",
    "            fps = 60 # 속력 60으로 고정\n",
    "            \n",
    "        #frameNumber = cap.get(cv2.CAP_PROP_FRAME_COUNT) # total frame numbers\n",
    "        # frames = cap.get(cv2.CAP_PROP_POS_FRAMES) # current frame numbers\n",
    "        \n",
    "        #img = cv2.imread('c:/OpenCV/image-003.jpeg')\n",
    "        \n",
    "        i = 0        \n",
    "        #stitcher = Stitcher()\n",
    "        result = []\n",
    "        \n",
    "        while cap.isOpened(): # isOpened() 동영상 파일 열기 성공 여부 확인\n",
    "            ret, frame = cap.read() # 비디오의 한프레임씩 읽음 \n",
    "                                    # 제대로 프레임 읽으면 ret = True, 아니면 ret = False\n",
    "                                    # frame은 읽은 프레임 나옴\n",
    "            if not ret :\n",
    "                break\n",
    "            \n",
    "            curr_frame = IMP(frame)   \n",
    "            curr_outimg = curr_frame.impTransformer() # 정사영으로 변환하기\n",
    "            curr_cropimg = curr_outimg[0:250, 0:1920] # 그중 위에 제대로 있는 부분만 사용\n",
    "            curr_cropimg = cv2.rotate(curr_cropimg, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "            result.append(curr_cropimg)\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "            #cv2.imwrite('road'+str(i)+'.jpg', curr_cropimg)\n",
    "        \n",
    "            if result is None:\n",
    "                print('[info] homography could not computed')\n",
    "                break\n",
    "              \n",
    "          #  cv2.imwrite('outimg'+ str(i)+'.jpg',outimg)    \n",
    "        \n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2acfde-ee01-4359-b06d-c782d4289dee",
   "metadata": {},
   "source": [
    "## findHomography 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "988275e4-a5d4-4154-88cc-3f329896fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3) (480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img1 = cv2.imread(\"./images/building/building1.jpg\")\n",
    "img2 = cv2.imread(\"./images/building/building2.jpg\")\n",
    "print(img1.shape,img2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b65ecba-56a6-4fff-bf4e-1d2f5ee644f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_frame = IMP(img1)   \n",
    "curr_outimg = curr_frame.impTransformer() # 정사영으로 변환하기\n",
    "curr_cropimg = curr_outimg[0:250, 0:1920] # 그중 위에 제대로 있는 부분만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b582850a-c9df-4c03-b5c8-8a79a007cff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 1080, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_outimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2188747-d546-4197-bb69-b3fce1207469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 1080, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_cropimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19c437a0-85b6-4ade-9d3d-8274d043d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', curr_cropimg)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d6995a-9068-40a5-9d95-11e9cd7b7ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1deb2f-d6b0-42fe-81ec-64fd974396a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMP(img1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a4d5c0d-e711-4ae9-a956-3a7e2e6e1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher = Stitcher()\n",
    "result1 = stitcher.stitch(images1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21a6da63-b141-4aca-b65b-b1be0db4163b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 1280, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c502a271-0d70-4f26-9a99-e6e71d727a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 480)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img2.shape[1] + img1.shape[1], img2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a2e474f-f763-4f09-939d-34c2611d7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv2.imshow('image', result1)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91be085-c307-4547-88f1-92fd4efe3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', img2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de29efe-7637-4131-a922-90e30e2636f9",
   "metadata": {},
   "source": [
    "## 파노라마 고도화 - 속력변화에 대응하여 이미지 붙이기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f03dd1-5d80-43db-8652-89061ae0ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "#import os\n",
    "\n",
    "img = cv2.imread(\"./images/road/road1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba69bb4-36ff-4c44-bee9-884754aecd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 250, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e3802-c961-4465-8a70-e747696b2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Aug 24 08:21:42 2021\n",
    "\n",
    "@author: KONIDE\n",
    "\n",
    "코드 설명 \n",
    "- 원본 이미지에서 투시변환할 부분을 line으로 그리고 point 찍어서 미리 확인해보기\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import os\n",
    "\n",
    "path = \"C:/OpenCV/\"\n",
    "\n",
    "img = cv2.imread(path+'image-003.jpeg')\n",
    " \n",
    "\n",
    "class Line:\n",
    "    def __init__(self, data1, data2):\n",
    "        self.line1 = data1\n",
    "        self.line2 = data2\n",
    "        \n",
    "    def slope(self):\n",
    "        (x1, y1), (x2, y2) = self.line1\n",
    "        (x3, y3), (x4, y4) = self.line2\n",
    "  \n",
    "        if (y2-y1) == 0 :\n",
    "            print('Ys are equal, m1 = 0')\n",
    "            m1 = 0\n",
    "        else:\n",
    "            m1 = (float(y2)-y1)/(float(x2)-x1)\n",
    "        \n",
    "        if (y4-y3) == 0 :\n",
    "            print('Ys are equal, m2 = 0')\n",
    "            m2 = 0\n",
    "        else:\n",
    "            m2 = (float(y4)-y3)/(float(x4)-x3)\n",
    "            \n",
    "        return m1, m2\n",
    "                    \n",
    "    def yintercept(self, m1, m2):\n",
    "        (x1, y1), (x2, y2) = self.line1\n",
    "        (x3, y3), (x4, y4) = self.line2\n",
    "        \n",
    "        if m1 != 0 :\n",
    "            b1 = y1 - m1*x1\n",
    "        else :\n",
    "            b1 = y1\n",
    "            \n",
    "        if m2 != 0 :\n",
    "            b2 = y4 - m2*x4\n",
    "            \n",
    "        else: b2 = y4\n",
    "        \n",
    "        return b1, b2\n",
    "    \n",
    "    def findIntersect(self, m1,m2, b1, b2):\n",
    "        \n",
    "        if m1 != 0 | m2 != 0 :\n",
    "            px = (b2-b1) / (m1-m2)\n",
    "            py = (b2*m1 - b1*m2)/(m1-m2)\n",
    "        elif m1 == 0 :\n",
    "            px = (b1-b2)/m2\n",
    "            py = b1\n",
    "        elif m2 == 0 : \n",
    "            px = (b2-b1)/m1\n",
    "            py = b2 \n",
    "        else :  print('No points')\n",
    "        \n",
    "        return px, py\n",
    "        \n",
    "\n",
    "\n",
    "topHeight = 565\n",
    "height, width = img.shape[:2]\n",
    "left = [(960, 380), (0, 650)]\n",
    "right = [(960, 380), (1920, 650)]\n",
    "up =  [(0, topHeight), (width+1000, topHeight)]\n",
    "down =  [(-10000,height), (width+100000, height)]\n",
    "\n",
    "\n",
    "leftup = Line(left, up)\n",
    "m1, m2 = leftup.slope()\n",
    "b1, b2 = leftup.yintercept(m1,m2)\n",
    "p1x, p1y = leftup.findIntersect(m1,m2,b1,b2)\n",
    "print('point1 : ', p1x, p1y)\n",
    "\n",
    "leftdown = Line(left, down)\n",
    "m1, m2 = leftdown.slope()\n",
    "b1, b2 = leftdown.yintercept(m1,m2)\n",
    "p2x, p2y = leftdown.findIntersect(m1,m2,b1,b2)\n",
    "print('point2 : ', p2x, p2y)\n",
    "\n",
    "\n",
    "rightup = Line(right, up)\n",
    "m1, m2 = rightup.slope()\n",
    "b1, b2 = rightup.yintercept(m1,m2)\n",
    "p3x, p3y = rightup.findIntersect(m1,m2,b1,b2)\n",
    "print('point3 : ', p3x, p3y)\n",
    "\n",
    "rightdown = Line(right, down)\n",
    "m1, m2 = rightdown.slope()\n",
    "b1, b2 = rightdown.yintercept(m1,m2)\n",
    "p4x, p4y = leftup.findIntersect(m1,m2,b1,b2)\n",
    "print('point4 : ', p4x, p4y)\n",
    " \n",
    "\n",
    "yellow = (0,255,255)\n",
    "blue = (255,0,0)\n",
    "red = (255, 0, 255)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SCRIPT_SIMPLEX\n",
    "fontScale = 1\n",
    "tickness = 2\n",
    "\n",
    "img = cv2.line(img, left[0], left[1], red, 2)\n",
    "img = cv2.line(img, right[0], right[1], red, 2)\n",
    "img = cv2.line(img, up[0], up[1], red, 2)\n",
    "img = cv2.line(img, down[0], down[1], red, 2)\n",
    "img = cv2.putText(img, 'Point1', (int(p1x), int(p1y)), font, fontScale, yellow, tickness, cv2.LINE_AA)\n",
    "img = cv2.putText(img, 'Point2', (0, height-200), font, fontScale, blue, tickness, cv2.LINE_AA)\n",
    "img = cv2.putText(img, 'Point3', (int(p3x), int(p3y)), font, fontScale, blue, tickness, cv2.LINE_AA)\n",
    "img = cv2.putText(img, 'Point4', (int(width/2), int(height/2)), font, fontScale, blue, tickness, cv2.LINE_AA)\n",
    "\n",
    "dst = np.array([[0,0], [0, 565], [1080,0], [1080,565]], dtype=np.float32)\n",
    "src = np.array([ [p1x,p1y], [p2x,p2y], [p3x,p3y], [p4x,p4y]], dtype=np.float32)\n",
    "mtrx = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "\n",
    "cv2.imshow('input_image', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "outimg = cv2.warpPerspective(img, mtrx, (1080,560))\n",
    "print(outimg.shape)\n",
    "cv2.imwrite('c:/OpenCV/out_image.jpg', outimg)\n",
    "cv2.imshow('c:/OpenCV/out_image.jpg',outimg)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
